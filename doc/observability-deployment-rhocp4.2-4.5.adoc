= Observability with Open Liberty

The following document covers various topics for configuring and integrating your Open Liberty runtime with monitoring tools in the OpenShift cluster. This document has been tested with Red Hat OpenShift Container Platform (RHOCP) 4.2, 4.3, 4.4, and 4.5.

== How to deploy Kibana dashboards to monitor Open Liberty logging events

Kibana dashboards are provided for visualizing events from the Open Liberty runtime.

To leverage these dashboards the logging events must be emitted in JSON format to standard-out. For Open Liberty Operator, JSON logging is enabled by default. For information regarding how to configure an Open Liberty image with JSON logging please see link:++https://github.com/OpenLiberty/ci.docker#logging++[here].

Retrieve available Kibana dashboards tuned for Open Liberty logging events link:++https://github.com/OpenLiberty/open-liberty-operator/tree/master/deploy/dashboards/logging++[here].

For information regarding how to import Kibana dashboards see the official documentation link:++https://www.elastic.co/guide/en/kibana/5.6/loading-a-saved-dashboard.html++[here].

For effective management of logs emitted from applications, deploy your own Elasticsearch, Fluentd and Kibana (EFK) stack. For more information see the following link:++https://www.ibm.com/support/knowledgecenter/en/SSCSJL_4.2.x/guides/guide-app-logging-ocp-4.2/app-logging-ocp-4.2.html++[guide].

Command-line JSON parsers, like JSON Query tool (jq), can be used to create human-readable views of JSON-formatted logs. You can create an alias for your jq command in the format you like. In the following example, the logs are piped through grep to ensure that the message field is there before jq parses the line:

[source,sh]
----
alias prettylog="grep --line-buffered message | jq '.ibm_datetime + \"  \" + .loglevel + \"\\t\" + \" \" + .message' -r"
oc logs -f pod_name -n namespace | prettylog
----

== How to monitor your Liberty runtimes

A MicroProfile Metrics enabled Open Liberty runtime is capable of tracking and observing metrics from the JVM and Open Liberty server as well as tracking MicroProfile Metrics instrumented within a deployed application. The tracked metrics can then be scraped by Prometheus and visualized with Grafana.

=== MicroProfile Metrics

The following steps outline how to manually create and modify a server.xml to add the MicroProfile Metrics feature that will be built as part of your Open Liberty image. You can use any version of the MicroProfile Metrics feature. If you use a version below mpMetrics-2.3, you should also include the monitor-1.0 feature to add Liberty server metrics. MicroProfile Metrics feature versions starting from mpMetrics-2.3 automatically load monitor-1.0 as a dependency.

. Create an XML file named `server_mpMetrics.xml` with the following contents and place it in the same directory as your Dockerfile:
+
[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<server>
   <featureManager>
       <feature>mpMetrics-2.3</feature>
       <!-- include the monitor-1.0 feature below if you are using an mpMetrics version below 2.3
       <feature>monitor-1.0</feature> 
       --> 
   </featureManager>
   <quickStartSecurity userName="${env.username}" userPassword="${env.password}"/>
</server>
----
+
The above `server.xml` configuration secures access to the server with basic authentication using the `<quickStartSecurity>` element. The `<quickStartSecurity>` is used in the above example for simplicity. When configuring your server you may wish to use a link:++https://www.ibm.com/support/knowledgecenter/en/SSEQTP_liberty/com.ibm.websphere.wlp.doc/ae/twlp_sec_basic_registry.html++[basic registry] or an link:++https://www.ibm.com/support/knowledgecenter/en/SSEQTP_liberty/com.ibm.websphere.wlp.doc/ae/twlp_sec_ldap.html++[LDAP registry] for securing authenticated access to your server. When using Prometheus to scrape data from the `/metrics` endpoint only the _Service Monitor_ approach can be configured to negotiate authentication with the Open Liberty server.

+
The two environment variables in the `<quickStartSecurity>` element, `username` and `password`, are used to avoid hardcoded, clear text authentication credentials in the `server.xml`. Follow the steps in the following section, <<Using environment variables for basic authentication credentials>>, to set up these two environment variables with your username and password of choice.

. In your DockerFile, add the following line to copy the `server_mpMetrics.xml` file into the `configDropins/overrides` directory:
+
[source,Dockerfile]
----
COPY --chown=1001:0 server_mpMetrics.xml /config/configDropins/overrides/
----

==== Using environment variables for basic authentication credentials

The following steps outline how to set up two environment variables that are used in the `<quickStartSecurity>` element for basic authentication, `username` and `password`, with your username and password of choice after your Open Liberty image is deployed onto OpenShift. 

. Create a secret in OpenShift with your desired username and password in the same namespace as your application.
. Modify your OpenLibertyApplication Custom Resource (CR) to add the `envFrom` definition with your secret referenced; in the following example, replace `basic-auth` with your secret:
+
[source,yaml]
----
spec:
  envFrom:
   - secretRef:
      name: basic-auth
----
+
This `envFrom` configuration sets two environment variables for your application container, `username` and `password`,  using your secret's respective username and password values. 


=== Enabling Prometheus to scrape data


You will need to deploy Prometheus using the Prometheus Operator which will then utilize Service Monitors to monitor and scrape logs from target services. Details regarding how to deploy and configure Prometheus are link:++https://www.ibm.com/support/knowledgecenter/en/SSCSJL_4.2.x/guides/guide-app-monitoring-ocp4.2/guide-app-monitoring-ocp4.2.html#deploy-prometheus-prometheus-operator++[here].


After deploying Prometheus, you must configure your application's Service Monitor to use the basic authentication credentials specified in your `server.xml` when accessing the `/metrics` endpoint. Use the Service Monitor's `basicAuth` definition with a secret that contains those credentials; this should be the same secret you created earlier in Step 1 of <<Using environment variables for basic authentication credentials>>.

Add the following `basicAuth` section to your ServiceMonitor YAML, and replace `basic-auth` with your secret:
[source,yaml]
----
spec:
  endpoints:
  - basicAuth:
      password:
        name: basic-auth
        key: password
      username:
        name: basic-auth
        key: username
----


=== Visualizing your data with Grafana


There are IBM provided Grafana dashboards that leverage metrics from the JVM as well as from the Open Liberty runtime.  Details regarding how to deploy and configure Grafana are covered link:++https://www.ibm.com/support/knowledgecenter/en/SSCSJL_4.2.x/guides/guide-app-monitoring-ocp4.2/guide-app-monitoring-ocp4.2.html#deploy-grafana++[here].


You can find the access point of Grafana by running the following:


[source,sh]
----
# oc get routes -n grafana
NAME          HOST/PORT                                      PATH      SERVICES      PORT      TERMINATION   WILDCARD
grafana-ocp   grafana-ocp-grafana.apps.9.37.135.153.nip.io             grafana-ocp   <all>     reencrypt     None
----

The `grafana` value is the namespace that you deploy Grafana to.

Sample Open Liberty Grafana dashboards are available for servers using either mpMetrics-1.x or mpMetrics-2.x link:++https://github.com/OpenLiberty/open-liberty-operator/tree/master/deploy/dashboards/metrics++[here]. Look in the featureManager section of the server.xml for either the mpMetrics feature or the umbrella microProfile feature to determine which dashboard to use.

.Features
|===
|Umbrella Feature |  mpMetrics Feature | Dashboard
|microProfile-1.2 - microProfile 2.2 |mpMetrics-1.x|ibm-websphere-liberty-grafana-dashboard.json
|microProfile-3.0 |mpMetrics-2.x|       ibm-websphere-liberty-grafana-dashboard-metrics-2.0.json
|===

== How to use health info with service orchestrator


MicroProfile Health allows services to report their readiness and liveness statuses (i.e UP if it is ready or alive and DOWN if its not ready/alive) through two endpoints. The Health data will be available on the `/health/live` and `/health/ready` endpoints for the liveness checks and for the readiness checks, respectively.
Readiness check allows third party services to know if the service is ready to process requests or not. e.g., dependency checks, such as database connections, application initialization, etc.
Liveness check allows third party services to determine if the service is running. This means that if this procedure fails the service can be discarded (terminated, shutdown). It reports an individual service's status at the endpoints and indicates the overall status as UP if all the services are UP. A service orchestrator can then use these health check statuses to make decisions.


=== MicroProfile Health 2.x

 The following steps outline how to manually create and modify a server.xml to add the mpHealth-2.x feature that will be built as part of your Open Liberty image.


Configure mpHealth-2.x feature in server.xml:


. Create an XML file named `server_mpHealth.xml`, with the following contents and place it in the same directory as your DockerFile:
+
[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<server>
   <featureManager>
       <feature>mpHealth-2.2</feature>
   </featureManager>
</server>
----


. In your DockerFile, add the following line to copy the `server_mpHealth.xml` file into the `configDropins/overrides` directory:
+
[source,Dockerfile]
----
COPY --chown=1001:0 server_mpHealth.xml /config/configDropins/overrides/
----


== Configure the Kubernetes Liveness and Readiness Probes to use the MicroProfile Health REST Endpoints


Kubernetes provides liveness and readiness probes that are used to check the health of your containers. These probes can check certain files in your containers, check a TCP socket, or make HTTP requests.

Configure the readiness and liveness probe's fields to point to the MicroProfile Health REST endpoints.

=== For mpHealth-2.x


Modify the readiness and liveness probe's fields, if not configured, to point to the MicroProfile Health REST endpoints, in the OpenLibertyApplication Custom Resource (CR):


[source,yaml]
----
spec:
  applicationImage:
  ...
  readinessProbe:
    failureThreshold: 12
    httpGet:
      path: /health/ready
      port: 9443
      scheme: HTTPS
    initialDelaySeconds: 30
    periodSeconds: 2
    timeoutSeconds: 10
  livenessProbe:
    failureThreshold: 12
    httpGet:
      path: /health/live
      port: 9443
      scheme: HTTPS
    initialDelaySeconds: 30
    periodSeconds: 2
    timeoutSeconds: 10
...
----

== Enable storage for serviceability

Using the operator, you can enable the serviceability definition in your OpenLibertyApplication Custom Resource to create a PersistentVolumeClaim so that the logs from your application go to a single storage. Your cluster must either be configured to automatically bind the PersistentVolumeClaim to a PersistentVolume or you must bind it manually.

The `serviceability.size` definition in the following example will automatically create a PersistentVolumeClaim with the specified size and is shared between all pods of the OpenLibertyApplication instance. For more information on the serviceability definition provided by the operator, please see the following link:++https://github.com/OpenLiberty/open-liberty-operator/blob/master/doc/user-guide.adoc#storage-for-serviceability++[user guide].

Add the `serviceability.size` definition in your OpenLibertyApplication Custom Resource; the PersistentVolumeClaim should be created with the name `<application_name>-serviceability`:

[source,yaml]
----
spec:
  applicationImage:
  ...
  serviceability:
    size: 1Gi
----
